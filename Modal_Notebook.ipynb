{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Finetuning with Unsloth on Modal\n",
    "\n",
    "This notebook demonstrates how to finetune various models using Unsloth on the Modal platform. Use the dropdown menu below to select the model you want to finetune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "model_options = {\n",
    "    'Llama 3.1 8B': 'meta-llama/Llama-3.1-8B',\n",
    "    'Gemma 3 4B': 'unsloth/gemma-3-4b',\n",
    "    'Gemma 3n E4B': 'unsloth/gemma-3n-E4B',\n",
    "    'Mistral 7B Instruct v0.3': 'unsloth/mistral-7b-instruct-v0.3',\n",
    "    'Qwen2 4B Instruct': 'Qwen/Qwen2-4B-Instruct',\n",
    "    'Qwen2 8B': 'Qwen/Qwen2-8B',\n",
    "    'Qwen2 34B Thinking': 'Qwen/Qwen2-34B-Thinking',\n",
    "    'Yi 6B': '01-ai/Yi-6B'\n",
    "}\n",
    "\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=model_options,\n",
    "    value='meta-llama/Llama-3.1-8B',\n",
    "    description='Model:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "display(model_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "import os\n",
    "\n",
    "# Modal App Configuration\n",
    "stub = modal.App(\"llama-finetune-unsloth\")\n",
    "\n",
    "# Modal Image Configuration\n",
    "image = modal.Image.debian_slim(python_version=\"3.10\") \\\n",
    "    .apt_install(\"git\") \\\n",
    "    .pip_install(\n",
    "        \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\",\n",
    "        \"torch\",\n",
    "        \"transformers\",\n",
    "        \"datasets\",\n",
    "        \"trl\",\n",
    "        \"accelerate\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@stub.function(\n",
    "    gpu=\"A10G\",\n",
    "    image=image,\n",
    "    secrets=[modal.Secret.from_name(\"my-huggingface-secret\")],\n",
    "    timeout=18000,\n",
    ")\n",
    "def finetune(model_name: str):\n",
    "    import torch\n",
    "    from huggingface_hub import login\n",
    "    from datasets import load_dataset\n",
    "    from unsloth import FastLanguageModel\n",
    "    from trl import SFTTrainer\n",
    "    from transformers import TrainingArguments\n",
    "\n",
    "    # Log in to Hugging Face\n",
    "    login(token=os.environ[\"HUGGING_FACE_HUB_TOKEN\"])\n",
    "\n",
    "    # Load the model and tokenizer first to apply the chat template\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=2048,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "\n",
    "    def format_prompt(example):\n",
    "        messages = example.get(\"messages\", [])\n",
    "        if messages:\n",
    "            try:\n",
    "                text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "                return {\"text\": text}\n",
    "            except Exception as e:\n",
    "                print(f\"Error formatting prompt: {e}\")\n",
    "                return {\"text\": \"\"}\n",
    "        else:\n",
    "            return {\"text\": \"\"}\n",
    "\n",
    "    # Load and format the dataset\n",
    "    dataset = load_dataset(\"Guilherme34/uncensor\", split=\"train\")\n",
    "    dataset = dataset.map(format_prompt, batched=False)  # Process one example at a time\n",
    "    # Filter out empty texts\n",
    "    dataset = dataset.filter(lambda x: len(x[\"text\"]) > 0)\n",
    "\n",
    "    # Configure LoRA\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=True,\n",
    "        random_state=3407,\n",
    "        use_rslora=False,\n",
    "        loftq_config=None,\n",
    "    )\n",
    "\n",
    "    # Configure the trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=2048,\n",
    "        dataset_num_proc=2,\n",
    "        packing=False,  # Can make training 5x faster for short sequences.\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=5,\n",
    "            max_steps=60,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=not torch.cuda.is_bf16_supported(),\n",
    "            bf16=torch.cuda.is_bf16_supported(),\n",
    "            logging_steps=1,\n",
    "            optim=\"adamw_8bit\",\n",
    "            weight_decay=0.01,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            seed=3407,\n",
    "            output_dir=\"outputs\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "\n",
    "    # Push the model to the Hugging Face Hub\n",
    "    safe_model_name = model_name.replace('/', '-')\n",
    "    hub_model_name = f\"realoperator42/{safe_model_name}-uncensored\"\n",
    "    model.push_to_hub(hub_model_name, token=os.environ[\"HUGGING_FACE_HUB_TOKEN\"])\n",
    "    print(f\"Model pushed to Hugging Face Hub at {hub_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Finetuning\n",
    "\n",
    "To run the finetuning process, you need to deploy this notebook to Modal. You can do this by running the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "modal deploy Modal_Notebook.ipynb\n",
    "```\n",
    "\n",
    "Then, to run the finetuning function, you can call it from the command line. Make sure to pass the selected model name:\n",
    "\n",
    "```bash\n",
    "modal run Modal_Notebook.ipynb::finetune --model-name [SELECTED_MODEL_NAME]\n",
    "```\n",
    "\n",
    "For example:\n",
    "```bash\n",
    "modal run Modal_Notebook.ipynb::finetune --model-name meta-llama/Llama-3.1-8B\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
